services:
  frontend:
    build:
      context: .
      dockerfile: ./reflex-app/Dockerfile
    ports:
      - "3000:3000"
      - "8000:8000"
    networks:
      - app-network
  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./results:/code/app/results
    environment:
      - HUGGING_FACE_TOKEN
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app-network
  llm:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    ports:
      - "4000:7000"
    volumes:
      - ./llm/models/llama-2-7b-chat.Q5_K_M.gguf:/models/llama-2-7b-chat.Q5_K_M.gguf
    command: [
      "-m", "/models/llama-2-7b-chat.Q5_K_M.gguf",
      "--port", "7000",
      "--host", "0.0.0.0",
      "-n", "1024",
      "--n-gpu-layers", "20",
      "--ctx-size", "4096",
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

