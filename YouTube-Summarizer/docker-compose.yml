version: "3.8"
services:
  frontend:
    build:
      context: .
      dockerfile: ./reflex-app/Dockerfile
    ports:
      - "3000:3000"
      - "8000:8000"
    networks:
      - app-network
  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    ports:
      - "8080:8080"
    environment:
      - HUGGING_FACE_TOKEN
    networks:
      - app-network
  llm:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - "4000:7000"
    volumes:
      - ./llm/models/llama-2-7b-chat.Q5_K_M.gguf:/models/llama-2-7b-chat.Q5_K_M.gguf
    command: [
      "-m", "/models/llama-2-7b-chat.Q5_K_M.gguf",
      "--port", "7000",
      "--host", "0.0.0.0",
      "-n", "512"
    ]
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

